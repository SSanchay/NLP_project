
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from joblib import load
import re
import string
import torch
import torch.nn as nn
from json import load as json_load
import transformers
import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from PIL import Image

path_folder = '/home/a_ladin/ds_offline/learning/project2/review_files/'

# –í–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω—ã –±—É–¥—É—Ç
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

## -------------------------- –î–ª—è 1-–æ–π –º–æ–¥–µ–ª–∏ ---------------------------

# –ö—ç—à–∏—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
sw = stopwords.words('english')
# –ó–∞–≥—Ä—É–∂–∞–µ–º vectorizer
tfid = load(path_folder + 'tfidf.joblib')
# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è sentiment_ML
model_ml = load(path_folder + 'logreg.joblib')


def clean(text):
    "–§—É–Ω–∫—Ü–∏—è —á–∏—Å—Ç–∏—Ç —Ç–µ–∫—Å—Ç"
    text = text.lower()  # –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    text = re.sub('<.*?>', '', text)  # Remove HTML from text
    text = text.translate(str.maketrans('', '', string.punctuation))  # —É–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'\d+', ' ', text)  # —É–¥–∞–ª—è–µ–º —á–∏—Å–ª–∞

    return text


def tokenize_review(review: str):
    "–§—É–Ω–∫—Ü–∏—è —á–∏—Å—Ç–∏—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞"
    cleaned_review = clean(review)  # cleaning text
    wn_lemmatizer = WordNetLemmatizer()  # lemmatization
    reg_tokenizer = RegexpTokenizer('\w+')  # tokenization

    lemmatized_review = ' '.join([wn_lemmatizer.lemmatize(word, tag[0].lower())  # –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å —É—á—ë—Ç–æ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏
                                  if tag[0].lower() in ['a', 'n', 'v']  # word - –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ adv, noun –∏ verb
                                  else wn_lemmatizer.lemmatize(word)  # –ø—Ä–æ—Å—Ç–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
                                  for word, tag in
                                  nltk.pos_tag(cleaned_review.split())])  # pos_tag - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á–∞—Å—Ç—å —Ä–µ—á–∏

    tokenized_review = reg_tokenizer.tokenize_sents([lemmatized_review])  # —Ä–µ–≤—å—é —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
    clean_tokenized_review = ' '.join(
        [word for word in tokenized_review[0] if word not in sw])  # —É–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, —Å–≤–µ—Ä—Ö—É –æ–±—ä—è–≤–∏–ª–∏ sw

    return clean_tokenized_review


## -------------------------- –î–ª—è 2-–æ–π –º–æ–¥–µ–ª–∏ ---------------------------

class sentimentGRU(nn.Module):
    def __init__(self,
                 vocab_size,  # –æ–±—ä—ë–º —Å–ª–æ–≤–∞—Ä—è —Å–ª–æ–≤
                 output_size,  # –Ω–µ–π—Ä–æ–Ω—ã –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è
                 embedding_dim,  # —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                 hidden_dim,  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Å–ª–æ—è LSTM
                 n_layers,  # —á–∏—Å–ª–æ —Å–ª–æ–µ–≤ –≤ LSTM
                 drop_prob=0.5):
        super().__init__()

        self.output_size = output_size
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        self.gru = nn.GRU(embedding_dim,
                          hidden_dim,
                          n_layers,
                          dropout=drop_prob,
                          batch_first=True)

        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(hidden_dim, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, hidden):
        batch_size = x.size(0)
        embeds = self.embedding(x)

        gru_out, hidden = self.gru(embeds, hidden)
        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)

        out = self.dropout(gru_out)
        out = self.fc(out)

        sig_out = self.sigmoid(out)
        sig_out = sig_out.view(batch_size, -1)
        sig_out = sig_out[:, -1]

        return sig_out, hidden

    def init_hidden(self, batch_size):
        "Hidden state –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω—É–ª—è–º–∏"
        hidden = torch.zeros((self.n_layers), batch_size, self.hidden_dim)

        return hidden


# –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤–æ-—á–∏—Å–ª–æ
with open(path_folder + 'dict.json', 'r') as fp:
    vocab_to_int = json_load(fp)

# –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è sentiment_RNN
vocab_size = len(vocab_to_int) + 1  # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è vocab_to_int
output_size = 1  # –∑–∞–¥–∞—á–∞ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
embedding_dim = 32  # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞
hidden_dim = 16  # —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏
n_layers = 2  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GRU —Å–ª–æ–µ–≤

model_rnn = sentimentGRU(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)
model_rnn.load_state_dict(torch.load(path_folder + 'params_gru.pt', map_location=torch.device('cpu')))
model_rnn.eval()

## -------------------------- –î–ª—è 3-–æ–π –º–æ–¥–µ–ª–∏ ---------------------------


# DistilBERT: ## –∑–∞–¥–∞–µ–º —Å–∞–º—É –º–æ–¥–µ–ª—å
model_class = transformers.DistilBertModel
## —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∫ –Ω–µ–π (–¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –±—É–¥–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è, —Å–º. –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ)
tokenizer_class = transformers.DistilBertTokenizer

## –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π
pretrained_weights = 'distilbert-base-uncased'
tokenizer_bert = tokenizer_class.from_pretrained(pretrained_weights)
model_bert = model_class.from_pretrained(pretrained_weights)
model_bert_ml = load(path_folder + 'log_reg_2.joblib')


# 1-–∞—è –º–æ–¥–µ–ª—å
def sentiment_ML(review: str):
    "—Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ ML-–º–æ–¥–µ–ª–∏"

    clean_tokenized_review = tokenize_review(review)
    tfid_representation = tfid.transform(
        [clean_tokenized_review])  # —Ä–µ–≤—å—é —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤, —Å–≤–µ—Ä—Ö—É –∑–∞–≥—Ä—É–∂–∞–µ–º vectorizer

    pred_proba = model_ml.predict_proba(tfid_representation)  # —Ö—Ä–∞–Ω–∏—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤

    return np.round(pred_proba[0][1], 3)  # –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å 1 –∫–ª–∞—Å—Å–∞ (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π)


# 2-–∞—è –º–æ–¥–µ–ª—å
def sentiment_RNN(review: str):
    "—Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ RNN-–º–æ–¥–µ–ª–∏"

    clean_tokenized_review = tokenize_review(review)

    num_review = []  # –∑–¥–µ—Å—å –±—É–¥—É—Ç –≤–º–µ—Å—Ç–æ —Å–ª–æ–≤ —á–∏—Å–ª–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è vocab_to_int #–≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º
    for word in clean_tokenized_review.split():
        try:
            num_review.append(vocab_to_int[word])
        except KeyError as e:
            print(f'Word {word} not in dictionary!')

    # padding
    padding_review = num_review[-200:]  # –æ–±—Ä—É–±–∞–µ–º –µ—Å–ª–∏ –±–æ–ª—å—à–µ 200 —Å–ª–æ–≤
    if len(num_review) <= 200:
        padding_review = list(np.zeros(200 - len(num_review))) + num_review  # –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –º–µ–Ω—å—à–µ 200 —Å–ª–æ–≤

    tensor_review = torch.Tensor(padding_review).long().unsqueeze(0)  # —Å–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä —Ä–µ–≤—å—é –¥–ª—è –º–æ–¥–µ–ª–∏
    test_h = model_rnn.init_hidden(1)  # —Å–æ–∑–¥–∞–µ–º hidden_state

    pred = model_rnn(tensor_review, test_h)
    pred_proba = pred[0].item()

    return np.round(pred_proba, 3)  # –≤—ã–¥–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å 1 –∫–ª–∞—Å—Å–∞ (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π)


# 3-–∞—è –º–æ–¥–µ–ª—å
def sentiment_BERT_ML(review: str):
    "—Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é BERT –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ ML-–º–æ–¥–µ–ª–∏"
    max_len = 64  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

    # –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenized = tokenizer_bert.encode(review,
                                      add_special_tokens=True, truncation=True,
                                      max_length=max_len)  # –¥–æ–±–∞–≤–∏–ª–∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –æ–±—Ä–µ–∑–∞–ª–∏ –ø–æ –º–∞–∫—Å –¥–ª–∏–Ω–µ

    padded = np.array(tokenized + [0] * (max_len - len(tokenized)))  # –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –º–µ–Ω—å—à–µ 64 —Å–ª–æ–≤

    attention_mask = np.where(padded != 0, 1, 0)  # –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ

    # –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    input_ids = torch.tensor(padded).unsqueeze(0)
    attention_mask = torch.tensor(attention_mask).unsqueeze(0)

    with torch.inference_mode():
        last_hidden_states = model_bert(input_ids, attention_mask=attention_mask)

    features = last_hidden_states[0][:, 0, :].numpy()

    pred_proba = model_bert_ml.predict_proba(features)

    return np.round(pred_proba[0][1], 3)  # –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å 1 –∫–ª–∞—Å—Å–∞ (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π)


movie_names = {'–ó–µ–ª–µ–Ω–∞—è –∫–Ω–∏–≥–∞': '2',
               '–ü—Ä–æ–ª–µ—Ç–∞—è –Ω–∞–¥ –≥–Ω–µ–∑–¥–æ–º –∫—É–∫—É—à–∫–∏': '1',
               '–ú—Å—Ç–∏—Ç–µ–ª–∏: –≥–¥–µ –¢–∞–Ω–æ—Å —É–±–∏–ª –ø–æ–ª–æ–≤–∏–Ω—É': '5',
               '–°—É–º–µ—Ä–∫–∏: –≥–¥–µ –ë–µ–ª–ª–∞ —Å—Ç–∞–ª–∞ –≤–∞–º–ø–∏—Ä–æ–º': '4',
               '–ù—É, –ø–æ–≥–æ–¥–∏!': '3'}


st.set_page_config(
    page_title="Film review predictions",
    page_icon="üëã",
)

st.title("Film review predictions")
selected_movie = st.selectbox("Select movie", movie_names.keys())
image = Image.open(path_folder + movie_names[selected_movie] + '.jpg')
st.image(image, caption=selected_movie)
my_input = st.text_input('–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç')
if my_input:
    plt.rcdefaults()
    fig, ax = plt.subplots()
    clist = [(0, "red"), (0.125, "red"), (0.25, "orange"), (0.5, "green"),
             (0.7, "green"), (0.75, "blue"), (1, "blue")]
    rvb = mcolors.LinearSegmentedColormap.from_list("", clist)
    model = ('Classic ML', 'LSTM', 'BERT')
    y_pos = np.arange(len(model))
    prob = np.array([sentiment_ML(my_input), sentiment_RNN(my_input), sentiment_BERT_ML(my_input)])

    fig.set_figwidth(4)
    fig.set_figheight(1)

    ax.barh(y_pos, prob, align='center', color=rvb(prob))
    ax.set_yticks(y_pos, labels=model)
    ax.set_xlim(left=0, right=1)
    ax.invert_yaxis()
    ax.set_xlabel('Positivity')
    ax.set_title('Model predictions')

    st.pyplot(fig)

